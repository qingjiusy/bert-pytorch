{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# prepare the dataset\n",
        "simple input: the conversation between two people"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "j2KIji9Ts0Wn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[35, 12, 24, 27, 16, 21, 9],\n",
              " [35, 9, 31, 15, 36, 20, 5, 13, 37, 27],\n",
              " [5, 37, 27, 10, 12, 24, 27, 30],\n",
              " [7, 31, 11, 22, 6, 17, 23],\n",
              " [14, 32, 20],\n",
              " [33, 27, 9],\n",
              " [18, 24, 27, 39, 30],\n",
              " [16, 21, 39, 29, 25, 34, 27],\n",
              " [16, 21, 39, 13, 28, 31, 8, 38, 36, 19, 4, 26]]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "  code by wmathor, modify by qingjiu\n",
        "  Reference : https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
        "         https://github.com/JayParks/transformer, https://github.com/dhlee347/pytorchic-bert\n",
        "'''\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "from random import *\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as Data\n",
        "\n",
        "text = (\n",
        "    'Hello, how are you? I am Romeo.\\n' # R\n",
        "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n' # J\n",
        "    'Nice meet you too. How are you today?\\n' # R\n",
        "    'Great. My baseball team won the competition.\\n' # J\n",
        "    'Oh Congratulations, Juliet\\n' # R\n",
        "    'Thank you Romeo\\n' # J\n",
        "    'Where are you going today?\\n' # R\n",
        "    'I am going shopping. What about you?\\n' # J\n",
        "    'I am going to visit my grandmother. she is not very well' # R\n",
        ")\n",
        "\n",
        "# use Regular Expressions to get a list 用正则表达式获得一个list\n",
        "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n') # filter '.', ',', '?', '!','-'\n",
        "word_list = list(set(\" \".join(sentences).split())) # ['hello', 'how', 'are', 'you',...]\n",
        "\n",
        "# build the dict\n",
        "word2idx = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
        "for i, w in enumerate(word_list):\n",
        "    word2idx[w] = i + 4\n",
        "idx2word = {i: w for i, w in enumerate(word2idx)}\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word2idx[s] for s in sentence.split()]\n",
        "    token_list.append(arr)\n",
        "\n",
        "# have a look at token_list\n",
        "token_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# hyper parameter\n",
        "`maxlen`: indicates that all sentences in the same batch are composed of 30 tokens, and PAD is used to make up for the insufficient tokens (the implementation here is relatively crude, and all sentences in all batches are directly fixed to 30)\n",
        "\n",
        "`max_pred`: indicates the maximum number of words that need to be predicted, that is, the cloze task in BERT\n",
        "\n",
        "`n_layers`: indicates the number of Encoder Layers\n",
        "\n",
        "`d_model`: indicates the dimensions of Token Embeddings, Segment Embeddings, and Position Embeddings\n",
        "\n",
        "`d_ff`: indicates the dimensions of the fully connected layer in the Encoder Layer\n",
        "\n",
        "`n_segments`: indicates how many sentences the Decoder input consists of\n",
        "\n",
        "`maxlen`: 表示同一个 batch 中的所有句子都由 30 个 token 组成，不够的补 PAD（这里我实现的方式比较粗暴，直接固定所有 batch 中的所有句子都为 30）\n",
        "\n",
        "`max_pred`: 表示最多需要预测多少个单词，即 BERT 中的完形填空任务\n",
        "\n",
        "`n_layers`: 表示 Encoder Layer 的数量\n",
        "\n",
        "`d_model`: 表示 Token Embeddings、Segment Embeddings、Position Embeddings 的维度\n",
        "\n",
        "`d_ff`: 表示 Encoder Layer 中全连接层的维度\n",
        "\n",
        "`n_segments`: 表示 Decoder input 由几句话组成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-AuXO3rQJIUj"
      },
      "outputs": [],
      "source": [
        "# BERT Parameters\n",
        "maxlen = 30 \n",
        "batch_size = 6\n",
        "max_pred = 5 # max tokens of prediction\n",
        "n_layers = 6\n",
        "n_heads = 12\n",
        "d_model = 768\n",
        "d_ff = 768*4 # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preprocessing\n",
        "It is necessary to randomly make or replace (hereinafter collectively referred to as `mask`) 15% of the `tokens` in a sentence according to probability, and also to splice any two sentences\n",
        "\n",
        "需要按照概率随机make或替换（以下统称为`mask`）一个句子中15%的`tokens`，并且拼接任意两个句子"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bOXYOwFAJH93"
      },
      "outputs": [],
      "source": [
        "# sample IsNext and NotNext to be same in small batch size\n",
        "def make_data():\n",
        "    batch = []\n",
        "    # positive 变量代表两句话是连续的个数\n",
        "    # negative 代表两句话不是连续的个数\n",
        "    # 在一个 batch 中，令这两个样本的比例为 1:1\n",
        "    positive = negative = 0\n",
        "\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        # random.randrange(stop)将生成一个在 [0, stop) 范围内的随机整数\n",
        "        # 随机选句子\n",
        "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange(len(sentences)) # sample random index in sentences\n",
        "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index] # 获得两个句子中的tokens\n",
        "        input_ids = [word2idx['[CLS]']] + tokens_a + [word2idx['[SEP]']] + tokens_b + [word2idx['[SEP]']]\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        # MASK LM\n",
        "        n_pred =  min(max_pred, max(1, int(len(input_ids) * 0.15))) # 15 % of tokens in one sentence\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
        "                          if token != word2idx['[CLS]'] and token != word2idx['[SEP]']] # candidate masked position\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)\n",
        "            masked_tokens.append(input_ids[pos])\n",
        "            # 80%变成[MASK]\n",
        "            if random() < 0.8:  # 80%\n",
        "                input_ids[pos] = word2idx['[MASK]'] # make mask\n",
        "            # 10%去随机变成字典中的任意值\n",
        "            elif random() > 0.9:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                while index < 4: # can't involve 'CLS', 'SEP', 'PAD'\n",
        "                  index = randint(0, vocab_size - 1)\n",
        "                input_ids[pos] = index # replace\n",
        "            # 剩余10%什么都不变\n",
        "\n",
        "        # Zero Paddings\n",
        "        # 把batch中的input列表填补到maxlen的长度\n",
        "        n_pad = maxlen - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # Zero Padding (100% - 15%) tokens\n",
        "        # 为了补齐 mask 的数量，因为不同句子长度，会导致不同数量的单词进行 mask，\n",
        "        # 我们需要保证同一个 batch 中，mask 的数量（必须）是相同的，所以也需要在后面补一些没有意义的东西，比方说 [0]\n",
        "        if max_pred > n_pred:\n",
        "            n_pad = max_pred - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        # 如果对应句子的idx相差一,那么两个句子就是连续的\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "    return batch\n",
        "# Proprecessing Finished"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# build Model\n",
        "Use pytorch to build Bert model \n",
        "\n",
        "prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bqxycRhzia7r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids.shape is torch.Size([6, 30]), masked_pos.shape is torch.Size([6, 5])\n"
          ]
        }
      ],
      "source": [
        "batch = make_data()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
        "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
        "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
        "print(\"input_ids.shape is {}, masked_pos.shape is {}\".format(input_ids.shape, masked_pos.shape))\n",
        "\n",
        "class MyDataSet(Data.Dataset):\n",
        "  def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
        "    self.input_ids = input_ids\n",
        "    self.segment_ids = segment_ids\n",
        "    self.masked_tokens = masked_tokens\n",
        "    self.masked_pos = masked_pos\n",
        "    self.isNext = isNext\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
        "\n",
        "# 获取dataset\n",
        "mydataset = MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext)\n",
        "# 使用现成的dataloader\n",
        "loader = Data.DataLoader(dataset=mydataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model's implement\n",
        "\n",
        "在神经网络的建模过程中，模型很重要的性质就是非线性，同时为了模型泛化能力，需要加入随机正则，例如`dropout`(随机置一些输出为0,其实也是一种变相的随机非线性激活)， 而随机正则与非线性激活是分开的两个事情， 而其实模型的输入是由非线性激活与随机正则两者共同决定的。\n",
        "\n",
        "`GELUs`(高斯误差线性单元(Gaussian Error Linear Unit))正是在激活中引入了随机正则的思想，是一种对神经元输入的概率描述，直观上更符合自然的认识，同时实验效果要比`Relus`与`ELUs`都要好.`GELUs`其实是 `dropout`、`zoneout`、`Relus`的综合\n",
        "\n",
        "而其中torch.erf(x / math.sqrt(2.0)) 的范围是 -1 到 1\n",
        "\n",
        "get_attn_pad_mask 这个函数用于生成自注意力机制（Self-Attention Mechanism）中的注意力掩码（Attention Mask）。具体来说，这个函数会生成一个用于遮蔽填充位置（padding positions）的掩码，确保在计算注意力分数时这些填充位置不会对结果产生影响"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, seq_len = seq_q.size()\n",
        "    # eq(zero) is PAD token\n",
        "    # seq_q.data.eq(0) 会生成一个与 seq_q 形状相同的布尔张量，\n",
        "    # 当 seq_q 中的元素等于 0（通常表示 PAD token）时，对应位置的值为 True，否则为 False\n",
        "    pad_attn_mask = seq_q.data.eq(0).unsqueeze(1)  # [batch_size, 1, seq_len]\n",
        "    return pad_attn_mask.expand(batch_size, seq_len, seq_len)  # [batch_size, seq_len, seq_len]\n",
        "\n",
        "# 激活函数\n",
        "def gelu(x):\n",
        "    \"\"\"\n",
        "      Implementation of the gelu activation function.\n",
        "      For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "      0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "      Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0))) #范围[0, x]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 构造一个bert的Embedding类,其中包括了bert要对每个token使用的三种Embedding: \n",
        "# tok_embed, pos_embed和seg_embed\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long) #用于生成一个包含均匀间隔值的 1D 张量。类似于 Python 中的 range 函数\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # pos.shape: [seq_len] -> [batch_size, seq_len]\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg) #embedding.shape: [batch_size, seq_len,d_model]\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    # # 注意这里\n",
        "    # def forward(self, Q, K, V, attn_mask):\n",
        "    #     scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n",
        "    #     scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "    #     attn = nn.Softmax(dim=-1)(scores)\n",
        "    #     context = torch.matmul(attn, V)\n",
        "    #     return context\n",
        "\n",
        "    # attn_mask: [batch_size, n_heads, seq_len, seq_len]\n",
        "    # q_s, k_s, v_s: [batch_size, n_heads, seq_len, d_k]\n",
        "    def forward(self, q_s, k_s, v_s, attn_mask):\n",
        "        scores = q_s @ k_s.transpose(-1, -2) / np.sqrt(d_k) # scores : [batch_size, n_heads, seq_len, seq_len]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores) # attn : [batch_size, n_heads, seq_len, seq_len]\n",
        "        context = attn @ v_s # context: [batch_size, n_heads, seq_len, d_k]\n",
        "        return context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 多头注意力机制\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "        self.l1 = nn.Linear(n_heads * d_v, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.attention_cal_result = ScaledDotProductAttention()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size, seq_len, d_model], k: [batch_size, seq_len, d_model], v: [batch_size, seq_len, d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # 得到了q,k,v的分数(经过各自W矩阵的映射)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size, n_heads, seq_len, seq_len]\n",
        "\n",
        "        # context: [batch_size, n_heads, seq_len, d_v], attn: [batch_size, n_heads, seq_len, seq_len]\n",
        "        # 定义模型并前向传播\n",
        "        context = self.attention_cal_result(q_s, k_s, v_s, attn_mask) # context: [batch_size, n_heads, seq_len, d_k]\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size, seq_len, n_heads, d_v] -> [batch_size, seq_len, n_heads * d_v]\n",
        "        output = self.l1(context) #output: [batch_size, seq_len, d_model]\n",
        "        return self.norm(output + residual) # output: [batch_size, seq_len, d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 前馈网络\n",
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
        "        return self.fc2(gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encoder层\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size, seq_len, d_model]\n",
        "        return enc_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### torch.gather函数 \n",
        "out = torch.gather(input, dim, index)\n",
        "\n",
        "gather返回的out的形状和index一样\n",
        "\n",
        "#out[i][j][k] = input[index[i][j][k]][j][k] # dim=0\n",
        "\n",
        "#out[i][j][k] = input[i][index[i][j][k]][k] # dim=1\n",
        "\n",
        "#out[i][j][k] = input[i][j][index[i][j][k]] # dim=2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`optim.Adadelta` 是 PyTorch 中的一种优化器，它是 Adadelta 算法的实现。Adadelta 是一种自适应学习率方法，旨在解决 Adagrad 中学习率衰减过快的问题。Adadelta 通过限制累积窗口中的梯度更新来避免学习率迅速下降"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6inMS744xRwh"
      },
      "outputs": [],
      "source": [
        "# bert模型\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.activ2 = gelu\n",
        "        # fc2 is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        self.fc2 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.fc2.weight = embed_weight\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids) #output: [bach_size, seq_len, d_model]\n",
        "        # 得到了一个boolen掩码矩阵enc_self_attn_mask,其中true表示对应位置为0\n",
        "        # 而input_ids中0对应的就是|PAD|\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) # [batch_size, maxlen, maxlen]\n",
        "        for layer in self.layers:\n",
        "            # 在多层的attention layers中不停的计算output\n",
        "            # output: [batch_size, max_len, d_model]\n",
        "            output = layer(output, enc_self_attn_mask)\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled = self.fc(output[:, 0]) #output[:, 0]: [batch_size, d_model], h_pooled: [batch_size, d_model]\n",
        "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n",
        "\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) #masked_pos.shape: [batch_size, max_pred]->[batch_size, max_pred, d_model]\n",
        "        # 从 output 张量中提取掩码位置的特征\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model] max_pred前面定义为5\n",
        "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
        "        # 生成词汇表大小的 logits, 这些 logits 通常用于预测掩码位置的词汇\n",
        "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
        "        return logits_lm, logits_clsf\n",
        "\n",
        "model = BERT()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "colab_type": "code",
        "id": "ShYHlLr-wA_Q",
        "outputId": "304b6e61-d3ea-4659-e1d3-8beca0976876"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0010 loss = 0.674609\n",
            "Epoch: 0020 loss = 0.628631\n",
            "Epoch: 0030 loss = 0.577879\n",
            "Epoch: 0040 loss = 0.552309\n",
            "Epoch: 0050 loss = 0.440841\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(50):\n",
        "    for input_ids, segment_ids, masked_tokens, masked_pos, isNext in loader:\n",
        "      logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
        "      loss_lm = criterion(logits_lm.view(-1, vocab_size), masked_tokens.view(-1)) # for masked LM\n",
        "      loss_lm = (loss_lm.float()).mean()\n",
        "      loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
        "      loss = loss_lm + loss_clsf\n",
        "      if (epoch + 1) % 10 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "colab_type": "code",
        "id": "VMY0ypt8wC9H",
        "outputId": "2e4de07c-c757-473c-ca53-5bc983503feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, how are you? I am Romeo.\n",
            "Hello, Romeo My name is Juliet. Nice to meet you.\n",
            "Nice meet you too. How are you today?\n",
            "Great. My baseball team won the competition.\n",
            "Oh Congratulations, Juliet\n",
            "Thank you Romeo\n",
            "Where are you going today?\n",
            "I am going shopping. What about you?\n",
            "I am going to visit my grandmother. she is not very well\n",
            "================================\n",
            "['[CLS]', 'nice', 'meet', '[MASK]', 'too', 'how', 'are', 'you', 'today', '[SEP]', 'i', 'am', 'going', 'to', '[MASK]', 'my', 'grandmother', 'she', 'is', 'not', 'very', 'well', '[SEP]']\n",
            "masked tokens list :  [16, 28, 27]\n",
            "predict masked tokens list :  [16, 28, 27]\n",
            "isNext :  False\n",
            "predict isNext :  False\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[1]\n",
        "print(text)\n",
        "print('================================')\n",
        "print([idx2word[w] for w in input_ids if idx2word[w] != '[PAD]'])\n",
        "\n",
        "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
        "                 torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
        "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
        "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
        "\n",
        "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_clsf else False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT-Torch",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
